{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "961xR8LdcJQY"
   },
   "outputs": [],
   "source": [
    "# Small LLM / Notebook created by Javier Ideami (ideami.com)\n",
    "# Typical LLMs need many GPUs and millions of dollars to be trained\n",
    "# This code trains a small LLM with a single GPU and little GPU memory \n",
    "# Of course results are not like a chatGPT, but they are good enough to see how the LLM trains to go\n",
    "# from random combinations of letters to actual words and phrases that are sometimes decently coherent\n",
    "# GPT3 has 175 Billion parameters. GPT4 has many, many more.\n",
    "# This model has only 19 Million parameters with its default settings. That's why its perfect for learning \n",
    "# and experimenting\n",
    "\n",
    "# Official notebook #vj30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For GOOGLE COLAB and similar platform Users:\n",
    "#### Make sure to select a GPU in the online platform. Don't run this code with a CPU (it will be too slow)\n",
    "\n",
    "# If you are running this code locally, your GPU should be selected automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R1tUgAJccK6D",
    "outputId": "dbabcd5d-ad95-4518-a0f8-e04fddbce82c"
   },
   "outputs": [],
   "source": [
    "# uncomment and run the following installation lines ONLY if you havent installed these libraries already outside of the notebook\n",
    "#!pip install ipdb -q\n",
    "#!pip install tqdm -q\n",
    "#!pip install sentencepiece -q\n",
    "#!pip install wandb -q\n",
    "\n",
    "# And if you are not in Google Colab and you didn't yet install Pytorch, make sure to do it:\n",
    "# find the ideal pytorch installation command at https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "6VnNqwkhiU3n"
   },
   "outputs": [],
   "source": [
    "### Import necessary libraries\n",
    "\n",
    "import os, sys\n",
    "import ipdb # for debugging\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import platform, shutil # detect platform type\n",
    "import requests, zipfile, io \n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import sentencepiece as spm # For the tokenizer\n",
    "\n",
    "# These lines improve performance for Ampere Architecture (e.g: A100s)\n",
    "# torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
    "# torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
    "# Empty GPU cache memory\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    print(\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "G5q26l98govJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you seem to have already downloaded the files. If you wish to re-download them, delete the encoded_data.pt file\n"
     ]
    }
   ],
   "source": [
    "# Download necessary files and create necessary folders\n",
    "# wiki.txt - dataset: a tiny segment of the English Wikipedia\n",
    "# wiki_tokenizer.model: trained tokenizer file (in another notebook I show you how to produce this file)\n",
    "# wiki_tokenizer.vocab: trained tokenizer file (in another notebook I show you how to produce this file)\n",
    "# encoded_data.pt (dataset tokenized with the tokenizer)\n",
    "# I will explain how to produce encoded_data.pt - because it takes quite a bit to process, it's nice to have it in advance\n",
    "\n",
    "# NOTE: Downloading will take a while, be patient. You can refresh your folder from time to time to see when the files\n",
    "# have been created. If you have any problems downloading the files with this code, I have also added llm_train.zip\n",
    "# to the downloadable resources of this lecture (however, best option is to use this code, because then you don't need\n",
    "# to upload the files or do anything else)\n",
    "\n",
    "files_url = \"https://ideami.com/llm_train\"\n",
    "\n",
    "# Downloading proceeds if we detect that one of the key files to download is not present\n",
    "if not os.path.exists(f\"encoded_data.pt\"):\n",
    "    print(\"Downloading files using Python\")\n",
    "    response = requests.get(files_url)\n",
    "    zipfile.ZipFile(io.BytesIO(response.content)).extractall(\".\")\n",
    "else:\n",
    "    print(\"you seem to have already downloaded the files. If you wish to re-download them, delete the encoded_data.pt file\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fre7fXD0fVD9",
    "outputId": "04d590af-d8cc-4e93-fd10-60b97fc473d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: You will be using:  mps\n"
     ]
    }
   ],
   "source": [
    "# Set main parameters\n",
    "\n",
    "# ARCHITECTURE PARAMETERS\n",
    "batch_size= 8 # How many samples do we train at once (set as needed, typical range 8 to 128)\n",
    "              # 8 is good for a GPU with 4GB of memory, 128 is good for a GPU with 24GB of memory\n",
    "context=512 # Sequence length used for training, 512 is a good compromise for our level of resources\n",
    "embed_size=384 # Embedding size\n",
    "n_layers = 7 # Number of transformer layers\n",
    "n_heads = 7 # Number of heads within each layer\n",
    "BIAS = True # Do we want Bias parameters?\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "lr = 3e-4 # Initial learning rate\n",
    "dropout=0.05 # Dropout percentage\n",
    "weight_decay = 0.01 # Weight decay regularizer\n",
    "grad_clip = 1.0 # Gradient clipping to prevent gradient explosion\n",
    "\n",
    "# TRAINING parameters\n",
    "train_iters = 100000 # Maximum number of training iterations\n",
    "eval_interval=50 # How often do we evaluate the performance?\n",
    "eval_iters=3 # Number of iterations while we evaluate performance\n",
    "compile = False # Compile will accelerate performance in compatible systems\n",
    "load_pretrained = False # Do we want to load a pretrained model to continue training?\n",
    "\n",
    "checkpoint_dir = 'models/'  # Where do we store checkpoints?\n",
    "\n",
    "checkpoint_fn = \"latest.pt\" \n",
    "# Name of checkpoint file to be saved during training\n",
    "\n",
    "checkpoint_load_fn = \"latest.pt\" \n",
    "# Name of checkpoint file to be loaded when load_pretrained is True\n",
    "# You can load llm2.pt to experiment with a checkpoint that already reached 2.31 of loss\n",
    "\n",
    "dtype = torch.bfloat16 # our target internal data type\n",
    "\n",
    "# MODE\n",
    "# Do we want to run the model in inference mode?\n",
    "inference=False \n",
    "\n",
    "# DEVICE - Sets device to GPU or CPU (use GPU always)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "print(\"device: You will be using: \",device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "0Z_omi-4fW0s",
    "outputId": "3f8ecc39-b72d-4825-a78a-2705f66a7210"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/meinczinger/src/github/llm/llm_train/wandb/run-20241122_181249-db6ozbg2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/meinczinger-personal-use/llm_udemy/runs/db6ozbg2' target=\"_blank\">basic-gpt2024_11_22_18_12_49</a></strong> to <a href='https://wandb.ai/meinczinger-personal-use/llm_udemy' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/meinczinger-personal-use/llm_udemy' target=\"_blank\">https://wandb.ai/meinczinger-personal-use/llm_udemy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/meinczinger-personal-use/llm_udemy/runs/db6ozbg2' target=\"_blank\">https://wandb.ai/meinczinger-personal-use/llm_udemy/runs/db6ozbg2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LOGGING parameters\n",
    "# When you run this cell, it will ask you to enter your Wandb API Key, which you\n",
    "# can find at your account on https://wandb.ai/settings#api\n",
    "wandb_log = True\n",
    "wandb_project = \"llm_udemy\"\n",
    "wandb_run_name = \"basic-gpt\" + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name)\n",
    "\n",
    "# The first time you run this logging code set to True, the weights and biases library\n",
    "# will ask you for an API key. You can follow the instructions in the video, or you can\n",
    "# also simply click on a link that should appear when you run this cell, pointing to this\n",
    "# address: https://wandb.ai/authorize  \n",
    "# Going to that address will allow you to quickly get an API key as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CPrBqt7NhwnZ",
    "outputId": "85aad864-9cd1-4f74-fa76-63a24988a20f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " that was used to represent a team in an old TV show, The A-Team. A capital a is written \"A\". Use a capital A at the start of a sentence if writing.\n",
      "\n",
      "A is also a musical note, sometimes referred to as \"La\".\n",
      "\n",
      "The letter 'A' was in the Phoenician alphabet's aleph. This symbol came from a simple picture of an ox head. \n",
      "\n",
      "This Phoenician letter helped make the basic blocks of later types of the letter. The Greeks later modified this letter and used it as their letter alpha. The Greek alphabet was use\n"
     ]
    }
   ],
   "source": [
    "with open('../data/wiki.txt', 'r', encoding='utf-8') as f:\n",
    "    text=f.read()\n",
    "\n",
    "print(text[10000:10500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UDNcMXo_fals",
    "outputId": "8381a7a5-047a-425e-ece7-d958b18721e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab_size: 4096\n",
      "Encoding Decoding functions ready\n"
     ]
    }
   ],
   "source": [
    "# SENTENCEPIECE TOKENIZER\n",
    "\n",
    "# Load trained tokenizer\n",
    "# Make sure that \" model_file = \" is pointing to the right file\n",
    "sp = spm.SentencePieceProcessor(model_file='wiki_tokenizer.model')\n",
    "\n",
    "# Get the vocabulary size of our tokenizer\n",
    "vocab_size = sp.get_piece_size()\n",
    "print(f\"Tokenizer vocab_size: {vocab_size}\")\n",
    "\n",
    "# Create the encoding and decoding tokenizer functions\n",
    "encode = lambda s: sp.Encode(s)\n",
    "decode = lambda l: sp.Decode(l)\n",
    "\n",
    "# Test that encoding and decoding are working well\n",
    "print(decode(encode(\"Encoding Decoding functions ready\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "tA2mDSq_fhwC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved encoded data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x1/5s7srkw11qn8zndqtnz1w43r0000gn/T/ipykernel_58569/795341420.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load('encoded_data.pt')\n"
     ]
    }
   ],
   "source": [
    "# Tokenization of the dataset\n",
    "if os.path.exists(f\"encoded_data.pt\"):\n",
    "    # Load encoded data if you already saved it previously\n",
    "    print(\"Loading saved encoded data\")\n",
    "    data = torch.load('encoded_data.pt')\n",
    "else:\n",
    "    # If you still didn't encode and save the encoding, do it here\n",
    "    print(\"Encoding data\")\n",
    "    data = torch.tensor(encode(text), dtype=torch.long)\n",
    "    torch.save(data, 'encoded_data.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S4B1cPQGnJM0",
    "outputId": "9b5d5e9d-db9a-4411-ef20-2177f99bf469"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data: 59.21 Million | Training: 53.29 Million | Validation: 5.92 Million\n"
     ]
    }
   ],
   "source": [
    "data_size=len(data) # Get the size of the dataset\n",
    "\n",
    "spl = int(0.9*data_size) # set the split at 90%-10%\n",
    "train_data=data[:spl] # training data will be 90% of the dataset\n",
    "val_data=data[spl:] # validation data will be 10% of the dataset\n",
    "print(f'Total data: {data_size/1e6:.2f} Million | Training: {len(train_data)/1e6:.2f} Million | Validation: {len(val_data)/1e6:.2f} Million')\n",
    "\n",
    "# data[:30] : shows the first 30 token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "1EGi6Aevnjtp"
   },
   "outputs": [],
   "source": [
    "############## HELPER FUNCTIONS ###########################\n",
    "\n",
    "# Return a batch of either training or evaluation data\n",
    "def get_batch(split):\n",
    "    # BS = Batch Size / SL = Sequence Length or context length\n",
    "    data = train_data if split==\"train\" else val_data # Select the split\n",
    "    inds = torch.randint(len(data)-context, (batch_size,)) # (BS)\n",
    "    x = torch.stack([data[i: i+context] for i in inds]) # (BS,SL)\n",
    "    y = torch.stack([data[i+1: i+context+1] for i in inds]) # (BS,SL)\n",
    "\n",
    "    # Examples of what it returns\n",
    "    # # First 10 elements of first batch of inputs and labels\n",
    "    #x[0][:10] -> tensor([ 664,  278, 4031, 4056, 4065, 4062, 4062, 4051, 13, 13])\n",
    "    #y[0][:10] -> tensor([ 278, 4031, 4056, 4065, 4062, 4062, 4051,   13, 13, 4066])\n",
    "\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x,y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cKRu7PKctLIS",
    "outputId": "2b6861cf-fc98-4a3b-c91c-8233cb613d79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.837954  Million parameters\n"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "# Main Training Process\n",
    "#################################################################################\n",
    "\n",
    "# Main Setup\n",
    "from llm_models import ModelFactory\n",
    "from llm_models.tools import ModelParameters\n",
    "\n",
    "params_path = \"../config/model_params.yaml\"\n",
    "\n",
    "params = ModelParameters(params_path)\n",
    "model = ModelFactory(\"basic_gpt\", params, vocab_size)\n",
    "\n",
    "# model = GPT() # Instantiate LLM\n",
    "model = model.to(dtype) # Set the precision type\n",
    "model = model.to(device) # Move it to the right device\n",
    "\n",
    "# Torch.compile compiles a PyTorch model to an optimized version, aiming to improve runtime performance and efficiency.\n",
    "# Disable if your system doesn't support it\n",
    "if compile:\n",
    "    print(\"Torch :: Compiling model\")\n",
    "    model = torch.compile(model)\n",
    "\n",
    "\n",
    "# Print the number of parameters of our model (19 million in our case)\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \" Million parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "KvX0LI8HtR_h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 8.395833015441895, 'eval': 8.375}\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Loss\n",
    "@torch.no_grad()  # Prevent gradient calculation\n",
    "def calculate_loss():\n",
    "    out={}\n",
    "    model.eval()\n",
    "    for split in ['train','eval']:        \n",
    "        l=torch.zeros(eval_iters)  # Create a tensor of zeros the size of eval_iters\n",
    "        for i in range(eval_iters):\n",
    "            x,y=get_batch(split) # Get a new batch of data\n",
    "            _,loss=model(x,y)  # Calculate the loss\n",
    "            l[i]=loss  # Store the loss in the next position of tensor\n",
    "        out[split]=l.mean().item()  # Calculate the mean and extract the final value\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "l=calculate_loss()\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5X20ZTtOu9mJ",
    "outputId": "bcf6bf71-bf8a-4412-c9b0-283d528943d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mountain in my city is bofl� Dep least Republic Classacing movefield million D Mahrialudi another members Hockeyickavel lotoutoor George spirove�urbiss His Los retired, Sal hor researchuedart red multurn snircetting takenlet Ill\".rick Scotland bec him warviron agesu all codeones thb if med\n"
     ]
    }
   ],
   "source": [
    "# Generate a new sample\n",
    "@torch.no_grad()\n",
    "def generate_sample(input):\n",
    "    t1 = torch.tensor(encode(input), dtype=torch.long, device=device) # Tokenize string -> (tensor of ids)\n",
    "    t1 = t1[None,:]  # (1 , [size of ids])\n",
    "    newgen = model.generate(t1,max=64)[0].tolist() # call the generate method, limit output size\n",
    "    result=decode(newgen) # decode the result with the tokenizer to get back characters\n",
    "    print(f\"{result}\")\n",
    "\n",
    "generate_sample(\"The mountain in my city is\") # Generate a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "7z9sOljjvq2l"
   },
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Main Training Process\n",
    "#################################################################################\n",
    "\n",
    "# Set Weight Decay differently for different kinds of parameters\n",
    "# parameter dictionary where keys are parameter names, and values are the parameter themselves\n",
    "p_dict = {p_name: p for p_name, p in model.named_parameters() if p.requires_grad} # len: 370\n",
    "\n",
    "# isolate weight matrices as they benefit specially from weight decay\n",
    "weight_decay_p = [p for n, p in p_dict.items() if p.dim() >= 2]  # len: 171\n",
    "\n",
    "# isolate other parameters like bias parameters, that don't benefit from weight decay\n",
    "no_weight_decay_p = [p for n, p in p_dict.items() if p.dim() < 2] # len: 199\n",
    "\n",
    "# store the parameter types in a list of dictionaries\n",
    "optimizer_groups = [\n",
    "    {'params': weight_decay_p, 'weight_decay': weight_decay},\n",
    "    {'params': no_weight_decay_p, 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# Declare optimizer, it helps us compute gradients, update parameters, manage learning rate, apply weight decay\n",
    "optimizer = torch.optim.AdamW(optimizer_groups, lr=lr, betas=(0.9, 0.99))\n",
    "# betas: control the exponential moving averages of the gradient and its square,\n",
    "# which are essential components of the Adam and AdamW optimization algorithms.\n",
    "\n",
    "# Declare scheduler to change learning rate through the training\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_iters, eta_min=lr/10)\n",
    "# learning rate will descend till a minimum of a tenth of the lr\n",
    "\n",
    "start_iteration = 0\n",
    "best_val_loss = float('inf')  # Track best loss value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "YXt7xGMvwQ_5"
   },
   "outputs": [],
   "source": [
    "# Loading Checkpoints\n",
    "\n",
    "# Loads a previously saved checkpoint\n",
    "def load_checkpoint(path):\n",
    "    print(\"LLM - Loading model\")\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict']) # Load parameters\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict']) # Load optimizer state\n",
    "    iteration = checkpoint['iteration'] # In what iteration did we save the model?\n",
    "    loss = checkpoint['loss'] # What was the last loss value?\n",
    "    print(f\"Loaded iter {iteration} with loss {loss}\")\n",
    "    return iteration, loss\n",
    "\n",
    "################# OPTIONAL : LOAD A PREVIOUS CHECKPOINT\n",
    "if os.path.exists(f\"{checkpoint_dir}/{checkpoint_load_fn}\") and load_pretrained:\n",
    "    start_iteration, loss = load_checkpoint(checkpoint_dir + checkpoint_load_fn)\n",
    "    best_val_loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "B2KPyh1cwo3t"
   },
   "outputs": [],
   "source": [
    "#### INFERENCE MODE - Activate inference and then exit\n",
    "if inference==True:\n",
    "    model.eval()\n",
    "    while True:\n",
    "         qs = input(\"Enter text (q to quit) >>> \")\n",
    "         if qs == \"\":\n",
    "             continue\n",
    "         if qs == 'q':\n",
    "             break\n",
    "         generate_sample(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "2zUtal8zwsUl",
    "outputId": "435ac1e0-8bb5-45f6-d17a-8f81e4e444d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: train loss: 8.375 / val loss: 8.416666984558105\n",
      "The mountain in my city is reswar Delchestermercmercialsc State ath red Louisiana storyiana Clannandited Prime Timeic instrland F� char cant environment agre govern relationship stat borderbleewrid before same bec\"ley start spec campaign heavy Mil often priv Penn Ireland decoard retiredlase rights Aboutechn Desans oud animask\u0013\n",
      "[CHECKPOINT]: Saving with loss:  8.416666984558105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 50/100000 [00:46<23:14:27,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "50: train loss: 5.96875 / val loss: 6.041666507720947\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "###################### TRAINING #################################\n",
    "#################################################################\n",
    "\n",
    "try:\n",
    "    for i in tqdm(range(start_iteration, train_iters)):\n",
    "        xb,yb = get_batch(\"train\") # Get a new batch of data\n",
    "        logits,loss = model(xb,yb) # Run the LLM and get the logits and the loss\n",
    "\n",
    "        if (i % eval_interval==0 or i == train_iters-1): # Calculate the loss\n",
    "            l = calculate_loss()\n",
    "            print(f\"\\n{i}: train loss: {l['train']} / val loss: {l['eval']}\")\n",
    "\n",
    "            # We do a quick test so that we observe the evolution through the training\n",
    "            # Remember that we use a very small dataset which doesn't include all topics\n",
    "            generate_sample(\"The mountain in my city is\") # Generate a sample\n",
    "\n",
    "            if l['eval'] < best_val_loss: # If we improved the best loss, save a checkpoint\n",
    "                best_val_loss = l['eval']\n",
    "                print(\"[CHECKPOINT]: Saving with loss: \", best_val_loss)\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': best_val_loss,\n",
    "                    'iteration': i,\n",
    "                }, checkpoint_dir + checkpoint_fn)\n",
    "\n",
    "        if wandb_log:\n",
    "            wandb.log({\n",
    "                    \"loss/train\": l['train'],\n",
    "                    \"loss/val\": l['eval'],\n",
    "                    \"lr\": scheduler.get_last_lr()[0],\n",
    "                },\n",
    "                step = i)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True) # Reset gradients\n",
    "        loss.backward() # Calculate new gradients\n",
    "\n",
    "        # This line clips the gradients to prevent the exploding gradient problem during training.\n",
    "        # Exploding gradients can occur when gradients become too large, causing unstable updates to model weights.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
    "\n",
    "        optimizer.step() # Update the model parameters\n",
    "        scheduler.step() # Update the learning rate value\n",
    "\n",
    "    if wandb_log:\n",
    "        wandb.finish()\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted. Cleaning up...\")\n",
    "\n",
    "finally:\n",
    "    # Release GPU memory\n",
    "    torch.mps.empty_cache()\n",
    "    print(\"GPU memory released.\")\n",
    "\n",
    "if wandb_log:   \n",
    "    wandb.finish()\n",
    "torch.mps.empty_cache()\n",
    "\n",
    "# Code designed by Javier ideami\n",
    "# ideami.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rnYRMN1-xAtK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
