{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import ipdb\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import platform, shutil\n",
    "import requests, zipfile, io\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# tokenizer\n",
    "import sentencepiece as spm\n",
    "\n",
    "# torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# torch.backends.cudnn.allow_tf32 = True\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "torch.backends.mps.benchmark = True\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: You will be using:  cpu\n"
     ]
    }
   ],
   "source": [
    "# architecture parameters\n",
    "batch_size = 8\n",
    "context = 512\n",
    "embed_size = 384\n",
    "n_layers = 7\n",
    "n_heads = 7\n",
    "BIAS = True\n",
    "\n",
    "# hyperparameters\n",
    "lr = 3e-4\n",
    "dropout = 0.05 # regularization\n",
    "weight_decay = 0.01 # regularization\n",
    "grad_clip = 1.0\n",
    "\n",
    "\n",
    "# training parameters\n",
    "train_iters = 100000\n",
    "eval_interval = 50 # every 50th iteration is used as a validation step\n",
    "eval_iterations = 10 # during evaluation, use 10 samples and build their average\n",
    "compile = False # better pytorch perforamnce (works only on compatible systems)\n",
    "checkpoint_dir = \"models/\"\n",
    "checkpoint_fn = \"latest'.pt\"\n",
    "checkpoint_load_fn = \"latest.pt\" # from where to restart the training\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "# mode\n",
    "inference = False\n",
    "\n",
    "# device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('device: You will be using: ', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmeinczinger\u001b[0m (\u001b[33mmeinczinger-personal-use\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/meinczinger/src/github/llm/llm_train/wandb/run-20241115_123234-mr48x0l5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/meinczinger-personal-use/llm_udemy/runs/mr48x0l5' target=\"_blank\">llm_udemy-2024_11_15_12_32_30</a></strong> to <a href='https://wandb.ai/meinczinger-personal-use/llm_udemy' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/meinczinger-personal-use/llm_udemy' target=\"_blank\">https://wandb.ai/meinczinger-personal-use/llm_udemy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/meinczinger-personal-use/llm_udemy/runs/mr48x0l5' target=\"_blank\">https://wandb.ai/meinczinger-personal-use/llm_udemy/runs/mr48x0l5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# logging\n",
    "wandb_log = True\n",
    "wandb_project = 'llm_udemy'\n",
    "wandb_run_name = \"llm_udemy-\" + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " that was used to represent a team in an old TV show, The A-Team. A capital a is written \"A\". Use a capital A at the start of a sentence if writing.\n",
      "\n",
      "A is also a musical note, sometimes referred to as \"La\".\n",
      "\n",
      "The letter 'A' was in the Phoenician alphabet's aleph. This symbol came from a simple pictur\n"
     ]
    }
   ],
   "source": [
    "with open(\"wiki.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "print(text[10000:10300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab_size: 4096\n"
     ]
    }
   ],
   "source": [
    "# tokenizer\n",
    "\n",
    "# load a trained tokenizer\n",
    "sp = spm.SentencePieceProcessor(model_file='wiki_tokenizer.model')\n",
    "\n",
    "vocab_size = sp.get_piece_size()\n",
    "print(f\"Tokenizer vocab_size: {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[612, 370, 698, 265, 684]\n",
      "Once upon time\n"
     ]
    }
   ],
   "source": [
    "encode = lambda s: sp.Encode(s)\n",
    "decode = lambda l: sp.Decode(l)\n",
    "\n",
    "print(encode(\"Once upon  time\"))\n",
    "print(decode(encode(\"Once upon  time\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading encoded data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x1/5s7srkw11qn8zndqtnz1w43r0000gn/T/ipykernel_10713/1622721217.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(\"encoded_data.pt\")\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"encoded_data.pt\"):\n",
    "    print(\"loading encoded data\")\n",
    "    data = torch.load(\"encoded_data.pt\")\n",
    "else:\n",
    "    data = torch.tensor(encode(text), dtype=torch.long)\n",
    "    torch.save(data, \"encoded_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size: 59.211077 million | Training: 53.29 million | Validation: 5.92 million\n"
     ]
    }
   ],
   "source": [
    "# splitting the data\n",
    "data_size = len(data)\n",
    "spl = int(0.9*data_size)\n",
    "train_data = data[:spl]\n",
    "val_data = data[spl:]\n",
    "\n",
    "print(f\"Total size: {data_size/1e6} million | Training: {len(train_data)/1e6:.2f} million | Validation: {len(val_data)/1e6:.2f} million\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512]) torch.Size([8, 512])\n",
      "tensor([4086,  914, 4031, 4089, 4089, 2894, 4070,  307,  261, 2025])\n",
      "tensor([ 914, 4031, 4089, 4089, 2894, 4070,  307,  261, 2025,  594])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split==\"train\" else val_data\n",
    "    inds = torch.randint(high=len(data) - context, size=(batch_size,))\n",
    "    x = torch.stack([data[i:i+context] for i in inds])\n",
    "    y = torch.stack([data[i+1:i+context+1] for i in inds])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch(\"train\")\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "print(x[0][:10])\n",
    "print(y[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwadLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(embed_size, 6 * embed_size, bias=BIAS),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(6 * embed_size, embed_size, bias=BIAS),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.network(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.queries = nn.Linear(embed_size, head_size, bias=BIAS)\n",
    "        self.keys = nn.Linear(embed_size, head_size, bias=BIAS)\n",
    "        self.values = nn.Linear(embed_size, head_size, bias=BIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multihead(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size for _ in range(n_heads))])\n",
    "        self.combine = nn.Linear(head_size * n_heads, embed_size, bias=BIAS) # 378 -> 384 (embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([Head(x) for head in self.heads], dim=-1)\n",
    "        # Each head outputs (BS, SL, head_size)\n",
    "        x = self.combine(x) # (BS, SL, embed_size)\n",
    "        x = self.dropout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def ___init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        head_size = embed_size // n_heads\n",
    "        self.ma = Multihead(n_heads, head_size)\n",
    "        self.feed_forward = ForwardLayer(embed_size)\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.ma(self.ln1(x))\n",
    "        x = x + self.feed_forward(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positions = nn.Embedding(context, embed_size)\n",
    "        # self.blocks = nn.Sequential(*[Block(n_heads) for _ in range(n_layers)])\n",
    "        self.ln = nn.LayerNorm(embed_size)\n",
    "        self.final_linear = nn.Linear(embed_size, vocab_size, bias=BIAS)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, input, targets=None):\n",
    "        loss = None\n",
    "        BS, SL = input.shape\n",
    "        emb = self.embeddings(input)\n",
    "        pos = self.positions(torch.arange(SL, device=device))\n",
    "        x = emb + pos\n",
    "        # x = self.blocks(x)\n",
    "        x = self.ln(x)\n",
    "        logits = self.final_linear(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            BS, SL, VS = logits.shape\n",
    "            logits = logits.view(BS*SL, VS)\n",
    "            targets = targets.view(BS*SL)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, input, max=500):\n",
    "        for _ in range(max):\n",
    "            input = input[:, -context:]\n",
    "            logits, _ = self(input)\n",
    "            logits = logits[:, -1, :] # pick last probability\n",
    "            probs = F.softmax(logits, dim=-1) # dim indicates last dimension\n",
    "            next = torch.multinomial(probs, num_samples=1)\n",
    "            input = torch.cat((input, next), dim=-1)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.4375\n"
     ]
    }
   ],
   "source": [
    "x, y = get_batch(\"train\")\n",
    "\n",
    "model = GPT()\n",
    "model = model.to(dtype)\n",
    "model = model.to(device)\n",
    "\n",
    "logits, loss = model(x,y)\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Once upon a time Jan monthsociationplaylantulf Sil tweared Africahetiction religious althoughror Mc0ross difficultsoneter Japan feet Martin Serv courmosthen Miss turn�head ArE companies playersS Asia Derman playingborn keepouncil describform David inj kept\u0011 also Holausertain see view region costk\u0005aliael specialitiz\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate_sample(input):\n",
    "    t1 = torch.tensor(encode(input), dtype=torch.long, device=device)\n",
    "    t1 = t1[None, :]\n",
    "    newgen = model.generate(t1, max=64)[0].tolist()\n",
    "    result = decode(newgen)\n",
    "    print(f\"Result: {result}\")\n",
    "\n",
    "generate_sample(\"Once upon a time\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
